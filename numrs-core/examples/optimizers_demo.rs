//! Ejemplo: Optimizers - SGD, Adam, RMSprop, AdaGrad
//!
//! Compara diferentes optimizers en:
//! 1. RegresiÃ³n lineal simple
//! 2. ClasificaciÃ³n con red neuronal
//! 3. AnÃ¡lisis de convergencia

use anyhow::Result;
use numrs::{AdaGrad, Adam, Array, Optimizer, RMSprop, Tensor, SGD};
use std::cell::RefCell;
use std::rc::Rc;

fn main() -> Result<()> {
    println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
    println!("  âš¡ NumRs Optimizers Demo");
    println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");

    // ========================================================================
    // EJEMPLO 1: RegresiÃ³n lineal con SGD
    // ========================================================================
    println!("ğŸ“ˆ EJEMPLO 1: RegresiÃ³n lineal con SGD\n");
    println!("  Objetivo: aprender y = 3x + 2\n");

    // Datos de entrenamiento
    let x_data = vec![1.0, 2.0, 3.0, 4.0, 5.0];
    let y_data = vec![5.0, 8.0, 11.0, 14.0, 17.0]; // y = 3x + 2

    // ParÃ¡metros (wrapped en Rc<RefCell> para el optimizer)
    let w = Rc::new(RefCell::new(Tensor::new(
        Array::new(vec![1], vec![0.5]),
        true,
    )));
    let b = Rc::new(RefCell::new(Tensor::new(
        Array::new(vec![1], vec![0.0]),
        true,
    )));

    // SGD con momentum
    let mut optimizer = SGD::new(vec![w.clone(), b.clone()], 0.01, 0.9, 0.0);

    println!(
        "  Pesos iniciales: w={:.4}, b={:.4}",
        w.borrow().values()[0],
        b.borrow().values()[0]
    );
    println!("\n  Entrenando con SGD (momentum=0.9)...\n");

    let epochs = 100;
    for epoch in 0..epochs {
        let mut total_loss = 0.0;

        for (&x_val, &y_true) in x_data.iter().zip(y_data.iter()) {
            let x = Tensor::new(Array::new(vec![1], vec![x_val]), false);
            let y_target = Tensor::new(Array::new(vec![1], vec![y_true]), false);

            // Forward
            let y_pred = w.borrow().mul(&x)?.add(&b.borrow())?;
            let loss = y_pred.mse_loss(&y_target)?;
            total_loss += loss.values()[0];

            // Backward
            loss.backward()?;
        }

        // Update
        optimizer.step()?;
        optimizer.zero_grad();

        if epoch % 20 == 0 || epoch == epochs - 1 {
            println!(
                "    Epoch {:3}: loss={:.6}, w={:.4}, b={:.4}",
                epoch,
                total_loss / x_data.len() as f32,
                w.borrow().values()[0],
                b.borrow().values()[0]
            );
        }
    }

    println!(
        "\n  Resultado: w={:.4}, b={:.4}",
        w.borrow().values()[0],
        b.borrow().values()[0]
    );
    println!("  (Objetivo: w=3.000, b=2.000) âœ“\n");

    // ========================================================================
    // EJEMPLO 2: ComparaciÃ³n de optimizers
    // ========================================================================
    println!("âš”ï¸  EJEMPLO 2: ComparaciÃ³n de optimizers\n");
    println!("  Problema: optimizar f(x) = (x - 5)Â²\n");

    // FunciÃ³n objetivo: f(x) = (x - 5)Â²
    // MÃ­nimo en x = 5

    let optimizers_to_test = vec![
        ("SGD (lr=0.1)", "sgd"),
        ("SGD+Momentum (lr=0.1, m=0.9)", "sgd_momentum"),
        ("Adam (lr=0.1)", "adam"),
        ("RMSprop (lr=0.1)", "rmsprop"),
    ];

    for (name, opt_type) in optimizers_to_test {
        // Inicializar x en posiciÃ³n aleatoria
        let x = Rc::new(RefCell::new(Tensor::new(
            Array::new(vec![1], vec![0.5]),
            true,
        )));

        let mut optimizer: Box<dyn Optimizer> = match opt_type {
            "sgd" => Box::new(SGD::new(vec![x.clone()], 0.1, 0.0, 0.0)),
            "sgd_momentum" => Box::new(SGD::new(vec![x.clone()], 0.1, 0.9, 0.0)),
            "adam" => Box::new(Adam::with_lr(vec![x.clone()], 0.1)),
            "rmsprop" => Box::new(RMSprop::new(vec![x.clone()], 0.1, 0.99, 1e-8, 0.0, 0.0)),
            _ => unreachable!(),
        };

        print!("  {:<30} | ", name);

        // Entrenar
        for _ in 0..50 {
            let target = Tensor::new(Array::new(vec![1], vec![5.0]), false);
            let diff = x
                .borrow()
                .add(&target.mul(&Tensor::new(Array::new(vec![1], vec![-1.0]), false))?)?;
            let loss = diff.mul(&diff)?;

            loss.backward()?;
            optimizer.step()?;
            optimizer.zero_grad();
        }

        let final_x = x.borrow().values()[0];
        let error = (final_x - 5.0).abs();
        println!("x={:.4} (error: {:.4})", final_x, error);
    }

    println!();

    // ========================================================================
    // EJEMPLO 3: Learning Rates y Schedulers
    // ========================================================================
    println!("ğŸ“Š EJEMPLO 3: Learning Rates con Schedulers\n");
    println!("  Problema: f(x) = (x - 10)Â²\n");

    let learning_rates = vec![0.01, 0.05, 0.1, 0.5];

    println!("  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”");
    println!("  â”‚ Learn.R. â”‚ Final x     â”‚ Iterations â”‚");
    println!("  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤");

    for &lr in &learning_rates {
        let x = Rc::new(RefCell::new(Tensor::new(
            Array::new(vec![1], vec![1.0]),
            true,
        )));
        let mut optimizer = SGD::new(vec![x.clone()], lr, 0.0, 0.0);

        let mut iters = 0;
        for _ in 0..200 {
            let target = Tensor::new(Array::new(vec![1], vec![10.0]), false);
            let diff = x
                .borrow()
                .add(&target.mul(&Tensor::new(Array::new(vec![1], vec![-1.0]), false))?)?;
            let loss = diff.mul(&diff)?;

            loss.backward()?;
            optimizer.step()?;
            optimizer.zero_grad();
            iters += 1;

            // Check convergence
            if (x.borrow().values()[0] - 10.0).abs() < 0.01 {
                break;
            }
        }

        println!(
            "  â”‚  {:.2}    â”‚  {:.4}     â”‚     {:3}    â”‚",
            lr,
            x.borrow().values()[0],
            iters
        );
    }

    println!("  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n");

    println!("  ğŸ’¡ ObservaciÃ³n:");
    println!("     â€¢ lr muy bajo â†’ convergencia lenta");
    println!("     â€¢ lr muy alto â†’ puede oscilar");
    println!("     â€¢ lr Ã³ptimo ~ 0.1-0.5 para este problema\n");

    // ========================================================================
    // RESUMEN
    // ========================================================================
    println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
    println!("  âœ… RESUMEN: Optimizers Implementados");
    println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");

    println!("1. âœ“ SGD:");
    println!("   â†’ Con/sin momentum");
    println!("   â†’ Weight decay (L2 regularization)");
    println!("   â†’ Velocities tracking");

    println!("\n2. âœ“ Adam:");
    println!("   â†’ Adaptive learning rates");
    println!("   â†’ First & second moment estimates");
    println!("   â†’ Bias correction");

    println!("\n3. âœ“ RMSprop:");
    println!("   â†’ Running average of squared gradients");
    println!("   â†’ Adaptive per-parameter learning rates");

    println!("\n4. âœ“ AdaGrad:");
    println!("   â†’ Accumulated squared gradients");
    println!("   â†’ Automatic learning rate decay");

    println!("\n5. âœ“ Trait Optimizer:");
    println!("   â†’ step() para update automÃ¡tico");
    println!("   â†’ zero_grad() para limpiar");
    println!("   â†’ learning_rate() getter/setter");

    println!("\n6. âœ“ Learning Rate Schedulers:");
    println!("   â†’ StepLR (decay cada N steps)");
    println!("   â†’ ExponentialLR (decay exponencial)");

    println!("\nğŸ’¡ PrÃ³ximo paso: Fase 3 - Training API");
    println!("   â†’ Module trait para modelos");
    println!("   â†’ Trainer de alto nivel");
    println!("   â†’ Data loaders\n");

    Ok(())
}

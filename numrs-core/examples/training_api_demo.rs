//! Ejemplo: Training API Completo
//! 
//! Demuestra el uso de la API de alto nivel para training:
//! 1. Module trait con Linear y Sequential
//! 2. Dataset y batching automÃ¡tico
//! 3. Trainer con fit() de alto nivel
//! 4. ComparaciÃ³n Adam vs SGD

use numrs::{Array, Tensor, Module, Linear, Sequential, ReLU, Sigmoid};
use numrs::{Trainer, TrainerBuilder, Dataset, MSELoss, CrossEntropyLoss};
use anyhow::Result;

fn main() -> Result<()> {
    println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
    println!("  ğŸ“ NumRs Training API Demo");
    println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");
    
    // ========================================================================
    // EJEMPLO 1: RegresiÃ³n con Linear layer
    // ========================================================================
    println!("ğŸ“ˆ EJEMPLO 1: RegresiÃ³n simple con Linear\n");
    println!("  Objetivo: y = 2x + 3\n");
    
    // Crear modelo: Linear(1, 1)
    let model = Linear::new(1, 1)?;
    
    // Dataset: y = 2x + 3
    let train_inputs = vec![
        vec![1.0], vec![2.0], vec![3.0], vec![4.0], vec![5.0],
        vec![6.0], vec![7.0], vec![8.0], vec![9.0], vec![10.0],
    ];
    let train_targets = vec![
        vec![5.0], vec![7.0], vec![9.0], vec![11.0], vec![13.0],
        vec![15.0], vec![17.0], vec![19.0], vec![21.0], vec![23.0],
    ];
    
    let dataset = Dataset::new(train_inputs.clone(), train_targets.clone(), 2);
    
    // Trainer con Adam
    let mut trainer = TrainerBuilder::new(model)
        .learning_rate(0.05)
        .build_adam(Box::new(MSELoss));
    
    println!("  Entrenando con Adam (lr=0.05, batch_size=2)...\n");
    
    // Entrenar
    let history = trainer.fit(&dataset, None, 50, false)?;
    
    // Mostrar progreso cada 10 epochs
    for (epoch, (metrics, _)) in history.iter().enumerate() {
        if epoch % 10 == 0 || epoch == history.len() - 1 {
            println!("    Epoch {:2}: loss={:.6}", epoch, metrics.loss);
        }
    }
    
    println!("\n  âœ“ RegresiÃ³n completada!\n");
    
    // ========================================================================
    // EJEMPLO 2: ClasificaciÃ³n binaria con Sequential
    // ========================================================================
    println!("ğŸ¯ EJEMPLO 2: ClasificaciÃ³n binaria\n");
    println!("  Arquitectura: Sequential[Linear(2â†’4), ReLU, Linear(4â†’2)]\n");
    
    // Crear modelo secuencial
    let model = Sequential::new(vec![
        Box::new(Linear::new(2, 4)?),
        Box::new(ReLU),
        Box::new(Linear::new(4, 2)?),
    ]);
    
    // Dataset simple de clasificaciÃ³n
    let train_inputs = vec![
        vec![0.0, 0.0],
        vec![0.0, 1.0],
        vec![1.0, 0.0],
        vec![1.0, 1.0],
        vec![0.5, 0.5],
        vec![0.8, 0.2],
        vec![0.2, 0.8],
        vec![0.9, 0.9],
    ];
    
    // Targets: clase 0 si x+y < 1, clase 1 si x+y >= 1
    let train_targets = vec![
        vec![1.0, 0.0],  // clase 0
        vec![1.0, 0.0],  // clase 0
        vec![1.0, 0.0],  // clase 0
        vec![0.0, 1.0],  // clase 1
        vec![1.0, 0.0],  // clase 0
        vec![1.0, 0.0],  // clase 0
        vec![1.0, 0.0],  // clase 0
        vec![0.0, 1.0],  // clase 1
    ];
    
    let dataset = Dataset::new(train_inputs.clone(), train_targets.clone(), 4);
    
    // Trainer con SGD
    let mut trainer = TrainerBuilder::new(model)
        .learning_rate(0.1)
        .build_sgd(Box::new(MSELoss));
    
    println!("  Entrenando con SGD (lr=0.1, batch_size=4)...\n");
    
    let history = trainer.fit(&dataset, None, 100, false)?;
    
    for (epoch, (metrics, _)) in history.iter().enumerate() {
        if epoch % 20 == 0 || epoch == history.len() - 1 {
            println!("    Epoch {:3}: loss={:.6}", epoch, metrics.loss);
        }
    }
    
    println!("\n  âœ“ ClasificaciÃ³n completada!\n");
    
    // ========================================================================
    // EJEMPLO 3: Red mÃ¡s profunda con evaluaciÃ³n
    // ========================================================================
    println!("ğŸ§  EJEMPLO 3: Red profunda con train/val split\n");
    println!("  Arquitectura: 3 â†’ 8 â†’ 4 â†’ 1\n");
    
    let model = Sequential::new(vec![
        Box::new(Linear::new(3, 8)?),
        Box::new(ReLU),
        Box::new(Linear::new(8, 4)?),
        Box::new(ReLU),
        Box::new(Linear::new(4, 1)?),
    ]);
    
    // Dataset mÃ¡s grande
    let mut train_inputs = Vec::new();
    let mut train_targets = Vec::new();
    
    for i in 0..30 {
        let x = i as f32 * 0.1;
        let y = i as f32 * 0.05;
        let z = i as f32 * 0.02;
        train_inputs.push(vec![x, y, z]);
        train_targets.push(vec![x + y + z]);  // Simple suma
    }
    
    // Split train/val
    let val_inputs = train_inputs.split_off(24);
    let val_targets = train_targets.split_off(24);
    
    let train_dataset = Dataset::new(train_inputs, train_targets, 4);
    let val_dataset = Dataset::new(val_inputs, val_targets, 2);
    
    let mut trainer = TrainerBuilder::new(model)
        .learning_rate(0.01)
        .build_adam(Box::new(MSELoss));
    
    println!("  Entrenando con Adam (lr=0.01)...\n");
    println!("  â”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”");
    println!("  â”‚ Epoch â”‚ Train Loss  â”‚ Val Loss  â”‚");
    println!("  â”œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤");
    
    let history = trainer.fit(&train_dataset, Some(&val_dataset), 30, false)?;
    
    for (epoch, (train_metrics, val_metrics)) in history.iter().enumerate() {
        if epoch % 10 == 0 || epoch == history.len() - 1 {
            let val_loss = val_metrics.as_ref().map(|m| m.loss).unwrap_or(0.0);
            println!("  â”‚  {:3}  â”‚   {:.6}   â”‚  {:.6}  â”‚", 
                     epoch, train_metrics.loss, val_loss);
        }
    }
    
    println!("  â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n");
    println!("  âœ“ Training con validaciÃ³n completado!\n");
    
    // ========================================================================
    // EJEMPLO 4: ComparaciÃ³n de learning rates
    // ========================================================================
    println!("âš¡ EJEMPLO 4: Impacto del learning rate\n");
    
    let learning_rates = vec![0.001, 0.01, 0.1];
    
    println!("  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”");
    println!("  â”‚    LR     â”‚ Final Loss  â”‚ Converge?  â”‚");
    println!("  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤");
    
    for &lr in &learning_rates {
        let model = Linear::new(1, 1)?;
        
        let inputs = vec![vec![1.0], vec![2.0], vec![3.0]];
        let targets = vec![vec![2.0], vec![4.0], vec![6.0]];
        let dataset = Dataset::new(inputs, targets, 3);
        
        let mut trainer = TrainerBuilder::new(model)
            .learning_rate(lr)
            .build_sgd(Box::new(MSELoss));
        
        let history = trainer.fit(&dataset, None, 50, false)?;
        let final_loss = history.last().unwrap().0.loss;
        let converged = final_loss < 0.1;
        
        println!("  â”‚  {:.4}   â”‚   {:.6}   â”‚    {:4}    â”‚", 
                 lr, final_loss, if converged { "âœ“" } else { "âœ—" });
    }
    
    println!("  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n");
    
    // ========================================================================
    // RESUMEN
    // ========================================================================
    println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
    println!("  âœ… RESUMEN: Training API Implementado");
    println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");
    
    println!("1. âœ“ Module Trait:");
    println!("   â†’ forward() para propagaciÃ³n");
    println!("   â†’ parameters() para optimizaciÃ³n");
    println!("   â†’ train()/eval() para modos");
    
    println!("\n2. âœ“ Layers Disponibles:");
    println!("   â†’ Linear(in, out) - fully connected");
    println!("   â†’ Sequential - composiciÃ³n de layers");
    println!("   â†’ ReLU, Sigmoid - activaciones");
    
    println!("\n3. âœ“ Dataset & Batching:");
    println!("   â†’ Dataset::new(inputs, targets, batch_size)");
    println!("   â†’ get_batch() automÃ¡tico");
    println!("   â†’ Soporte para train/val split");
    
    println!("\n4. âœ“ Trainer API:");
    println!("   â†’ TrainerBuilder para construcciÃ³n");
    println!("   â†’ fit() con mÃºltiples epochs");
    println!("   â†’ ValidaciÃ³n automÃ¡tica opcional");
    println!("   â†’ MÃ©tricas (loss, accuracy)");
    
    println!("\n5. âœ“ Loss Functions:");
    println!("   â†’ MSELoss (regresiÃ³n)");
    println!("   â†’ CrossEntropyLoss (clasificaciÃ³n)");
    
    println!("\nğŸ’¡ CaracterÃ­sticas Completas:");
    println!("   â†’ Autograd âœ“ (gradientes automÃ¡ticos)");
    println!("   â†’ Optimizers âœ“ (SGD, Adam, RMSprop, AdaGrad)");
    println!("   â†’ Training API âœ“ (Module, Trainer, Dataset)");
    println!("   â†’ ONNX Support âœ“ (save/load modelos)");
    
    println!("\nğŸ‰ NumRs estÃ¡ listo para Machine Learning!");
    println!("   â†’ API similar a PyTorch");
    println!("   â†’ Backend de alto rendimiento (MKL, WebGPU)");
    println!("   â†’ Compatible con Rust ecosystem\n");
    
    Ok(())
}
